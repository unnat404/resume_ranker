{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing import docx_processing  as doc, textract_processing as txt\n",
    "# from text_processing import tf_idf_cosine_similarity as tf_idf,doc2vec_comparison as d2v\n",
    "# from text_processing import cv_cosine_similarity as cv\n",
    "# import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "import re\n",
    "\n",
    "def get_content_as_string(filename):\n",
    "    \"\"\"\n",
    "    use textract library to extract text from varied file formats like (.docx, .doc, .gif, .csv, .pdf, .png, ...etc) using a single api\n",
    "\n",
    "    :params:\n",
    "    filename : name of file to be extracted to text\n",
    "\n",
    "    :returns:\n",
    "    lower_case_string : text content of data in lowercase\n",
    "    \"\"\"\n",
    "    text = textract.process(filename)\n",
    "    lower_case_string =  str(text.decode('utf-8')).lower()\n",
    "    #final_string = re.sub('[^a-zA-Z0-9 \\n]', '', lower_case_string)\n",
    "    return lower_case_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Caution when deploying\n",
    "# TODO(done) : Download stopwords once initially\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove stop words and lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as stp\n",
    "# from preprocessing import lemma_tagger as tag\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    # return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in set(stp.words('english')))\n",
    "    return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in english_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords as stp\n",
    "\n",
    "\n",
    "def get_tf_idf_cosine_similarity(compare_doc,doc_corpus):\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # analyzer = TfidfVectorizer().build_analyzer()\n",
    "    #\n",
    "    # def stemmed_words(doc):\n",
    "    #     return (lemmatizer.lemmatize(w) for w in analyzer(doc) if w not in set(stp.words('english')))\n",
    "\n",
    "    tf_idf_vect = TfidfVectorizer(analyzer=stemmed_words)\n",
    "    #tf_idf_vect = TfidfVectorizer(stop_words='english')\n",
    "    tf_idf_req_vector = tf_idf_vect.fit_transform([compare_doc]).todense()\n",
    "    #tf_idf_req_vector = tf_idf_vect.fit_transform(doc_corpus).todense()\n",
    "    #print('Features are:', len(tf_idf_vect.get_feature_names()))\n",
    "    #print(tf_idf_vect.get_feature_names())\n",
    "    tf_idf_resume_vector = tf_idf_vect.transform(doc_corpus).todense()\n",
    "    #tf_idf_resume_vector = tf_idf_vect.transform([compare_doc]).todense()\n",
    "    cosine_similarity_list = []\n",
    "    for i in range(len(tf_idf_resume_vector)):\n",
    "        cosine_similarity_list.append(cosine_similarity(tf_idf_req_vector,tf_idf_resume_vector[i])[0][0])\n",
    "    # for i in range(len(tf_idf_req_vector)):\n",
    "    #   cosine_similarity_list.append(cosine_similarity(tf_idf_resume_vector,tf_idf_req_vector[i])[0][0])\n",
    "    return cosine_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(job_desc,resume_list):\n",
    "\n",
    "    job_desc_text = get_content_as_string(job_desc)\n",
    "    resume_list_text = []\n",
    "    for cur_resume in resume_list:\n",
    "        resume_list_text.append(get_content_as_string(cur_resume))\n",
    "\n",
    "    #similarity_list = d2v.get_doc2vec_similarity(req_doc_text, resume_doc_text)\n",
    "    #cos_sim_list = tf_idf.get_tf_cosine_similarity(req_doc_text,resume_doc_text)\n",
    "    # zipped_docs = zip(cos_sim_list,resume_docs)\n",
    "    # sorted_doc_list = sorted(zipped_docs, key = lambda x: x[0],reverse=True)\n",
    "    # final_doc_rating_list = []\n",
    "    # for element in similarity_list:\n",
    "    #     doc_rating_list = []\n",
    "    #     doc_rating_list.append(os.path.basename(resume_docs[element[0]]))\n",
    "    #     doc_rating_list.append(\"{:.0%}\".format(element[1]))\n",
    "    #     final_doc_rating_list.append(doc_rating_list)\n",
    "    #print(final_doc_rating_list)\n",
    "    #return final_doc_rating_list\n",
    "    # print('TF cosine similarity')\n",
    "    #print(final_doc_rating_list)\n",
    "\n",
    "\n",
    "\n",
    "    # TF-IDF - cosine similarity\n",
    "    # final_doc_rating_list = []\n",
    "    cos_sim_list = get_tf_idf_cosine_similarity(job_desc_text,resume_list_text)\n",
    "    final_doc_rating_list = []\n",
    "    zipped_docs = zip(cos_sim_list,resume_list)\n",
    "    sorted_doc_list = sorted(zipped_docs, key = lambda x: x[0], reverse=True)\n",
    "    for element in sorted_doc_list:\n",
    "        doc_rating_list = []\n",
    "        doc_rating_list.append(os.path.basename(element[1]))\n",
    "        doc_rating_list.append(\"{:.0%}\".format(element[0]))\n",
    "        final_doc_rating_list.append(doc_rating_list)\n",
    "    return final_doc_rating_list\n",
    "    # print('TF-IDF cosine similarity')\n",
    "    # print(final_doc_rating_list)\n",
    "    #\n",
    "    # # binary cv - cosine similarity\n",
    "    # final_doc_rating_list = []\n",
    "    # cos_sim_list = cv.get_binay_cosine_similarity(req_doc_text,resume_doc_text)\n",
    "    #\n",
    "    # zipped_docs = zip(cos_sim_list,resume_docs)\n",
    "    # sorted_doc_list = sorted(zipped_docs, key = lambda x: x[0],reverse=True)\n",
    "    # for element in sorted_doc_list:\n",
    "    #     doc_rating_list = []\n",
    "    #     doc_rating_list.append(os.path.basename(element[1]))\n",
    "    #     doc_rating_list.append(\"{:.0%}\".format(element[0]))\n",
    "    #     final_doc_rating_list.append(doc_rating_list)\n",
    "    # print('Binary BOW cosine similarity')\n",
    "    # print(final_doc_rating_list)\n",
    "    #\n",
    "    # # Regular cv - cosine similarity\n",
    "    # final_doc_rating_list = []\n",
    "    # cos_sim_list = cv.get_cosine_similarity(req_doc_text,resume_doc_text)\n",
    "    #\n",
    "    # zipped_docs = zip(cos_sim_list,resume_docs)\n",
    "    # sorted_doc_list = sorted(zipped_docs, key = lambda x: x[0],reverse=True)\n",
    "    # for element in sorted_doc_list:\n",
    "    #     doc_rating_list = []\n",
    "    #     doc_rating_list.append(os.path.basename(element[1]))\n",
    "    #     doc_rating_list.append(\"{:.0%}\".format(element[0]))\n",
    "    #     final_doc_rating_list.append(doc_rating_list)\n",
    "    # print(' BOW cosine similarity')\n",
    "    # print(final_doc_rating_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\UNNAT/nltk_data'\n    - 'c:\\\\Users\\\\UNNAT\\\\.conda\\\\envs\\\\resume_ranker_env\\\\nltk_data'\n    - 'c:\\\\Users\\\\UNNAT\\\\.conda\\\\envs\\\\resume_ranker_env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\UNNAT\\\\.conda\\\\envs\\\\resume_ranker_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\UNNAT\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8224\\1053558008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     ]\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mresult_ranking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_document\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_ranking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# lemmatizer = WordNetLemmatizer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8224\\1475467761.py\u001b[0m in \u001b[0;36mprocess_files\u001b[1;34m(job_desc, resume_list)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# TF-IDF - cosine similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# final_doc_rating_list = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mcos_sim_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tf_idf_cosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_desc_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_list_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mfinal_doc_rating_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mzipped_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcos_sim_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8224\\3346534063.py\u001b[0m in \u001b[0;36mget_tf_idf_cosine_similarity\u001b[1;34m(compare_doc, doc_corpus)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtf_idf_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstemmed_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#tf_idf_vect = TfidfVectorizer(stop_words='english')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtf_idf_req_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompare_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m#tf_idf_req_vector = tf_idf_vect.fit_transform(doc_corpus).todense()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#print('Features are:', len(tf_idf_vect.get_feature_names()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8224\\1902580264.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstemmed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in set(stp.words('english')))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8224\\1902580264.py\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;34m\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     tag_dict = {\"J\": wordnet.ADJ,\n\u001b[0;32m     14\u001b[0m                 \u001b[1;34m\"N\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             )\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\UNNAT/nltk_data'\n    - 'c:\\\\Users\\\\UNNAT\\\\.conda\\\\envs\\\\resume_ranker_env\\\\nltk_data'\n    - 'c:\\\\Users\\\\UNNAT\\\\.conda\\\\envs\\\\resume_ranker_env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\UNNAT\\\\.conda\\\\envs\\\\resume_ranker_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\UNNAT\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    req_document = r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\job_description_samples\\amazon_data_scientist_ii.txt'\n",
    "    resume_docs = [\n",
    "        # r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\Unnat_Resume.pdf',\n",
    "        r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\backend_developer_resume_amazon.png',\n",
    "        r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\infra_sec_resume.png'\n",
    "        # r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\data_scientist_resume_example.pdf'\n",
    "    ]\n",
    "\n",
    "    result_ranking = process_files(req_document,resume_docs)\n",
    "    print(result_ranking)\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # word_list = ['lead', 'leadership', 'leading']\n",
    "    # op = ' '.join(lemmatizer.lemmatize(w) for w in word_list)\n",
    "    # print(op)\n",
    "\n",
    "    # text = textract.process(r\"C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\Unnat_Resume.pdf\", method=\"pdfminer\").decode(\"utf8\")\n",
    "    # print(text)\n",
    "       \n",
    "    print(\"------------------------\")\n",
    "    print(\"Current Job Description\")\n",
    "    print(\"------------------------\")\n",
    "    print(get_content_as_string(req_document))\n",
    "    \n",
    "    print(\"------------------------\")\n",
    "    print(\"Current Resumes\")\n",
    "    print(\"------------------------\")\n",
    "    for ind,resume in enumerate(resume_docs):\n",
    "        print(f\"Resume {ind}\\n------------------------\")\n",
    "        print(get_content_as_string(resume))\n",
    "\n",
    "    # TODO : issue with converting pdf to text using textract (fix it) \n",
    "    # https://stackoverflow.com/questions/63359767/textract-failed-with-exit-code-127-pdftotext-on-windows-10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('resume_ranker_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdd37cec0271e5f9301f878c6561a7ffdc9e6fc062c1e94414084ed432e13a60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
