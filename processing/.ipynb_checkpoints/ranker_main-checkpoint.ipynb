{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as stp\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords as stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Caution when deploying\n",
    "# # TODO(done) : Download from nltk: {stopwords,averaged_perceptron_tagger,wordnet,omw-1.4} :  once initially\n",
    "# # import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "import re\n",
    "\n",
    "def get_content_as_string(filename):\n",
    "    \"\"\"\n",
    "    use textract library to extract text from varied file formats like (.docx, .doc, .gif, .csv, .pdf, .png, ...etc) using a single api\n",
    "\n",
    "    :params:\n",
    "    filename : name of file to be extracted to text\n",
    "\n",
    "    :returns:\n",
    "    lower_case_string : text content of data in lowercase\n",
    "    \"\"\"\n",
    "    text = textract.process(filename)\n",
    "    lower_case_string =  str(text.decode('utf-8')).lower()\n",
    "    #final_string = re.sub('[^a-zA-Z0-9 \\n]', '', lower_case_string)\n",
    "    return lower_case_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UNNAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\UNNAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UNNAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\UNNAT\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# Caution when deploying\n",
    "# TODO(done) : Download stopwords once initially\n",
    "# import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove stop words and lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as stp\n",
    "# from preprocessing import lemma_tagger as tag\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    # return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in set(stp.words('english')))\n",
    "    return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in english_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords as stp\n",
    "\n",
    "\n",
    "def get_tf_idf_cosine_similarity(compare_doc,doc_corpus):\n",
    "    \n",
    "    tf_idf_vect = TfidfVectorizer(analyzer=stemmed_words)\n",
    "    tf_idf_req_vector = tf_idf_vect.fit_transform([compare_doc]).todense()\n",
    "    #tf_idf_req_vector = tf_idf_vect.fit_transform(doc_corpus).todense()\n",
    "    #print('Features are:', len(tf_idf_vect.get_feature_names()))\n",
    "    #print(tf_idf_vect.get_feature_names())\n",
    "    tf_idf_resume_vector = tf_idf_vect.transform(doc_corpus).todense()\n",
    "    #tf_idf_resume_vector = tf_idf_vect.transform([compare_doc]).todense()\n",
    "    cosine_similarity_list = []\n",
    "    for i in range(len(tf_idf_resume_vector)):\n",
    "        cosine_similarity_list.append(cosine_similarity(tf_idf_req_vector,tf_idf_resume_vector[i])[0][0])\n",
    "    # for i in range(len(tf_idf_req_vector)):\n",
    "    #   cosine_similarity_list.append(cosine_similarity(tf_idf_resume_vector,tf_idf_req_vector[i])[0][0])\n",
    "    return cosine_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(job_desc,resume_list):\n",
    "\n",
    "    job_desc_text = get_content_as_string(job_desc)\n",
    "    resume_list_text = []\n",
    "    for cur_resume in resume_list:\n",
    "        resume_list_text.append(get_content_as_string(cur_resume)) \n",
    "\n",
    "    # TF-IDF - cosine similarity\n",
    "    # final_doc_rating_list = []\n",
    "    cos_sim_list = get_tf_idf_cosine_similarity(job_desc_text,resume_list_text)\n",
    "    final_doc_rating_list = []\n",
    "    zipped_docs = zip(cos_sim_list,resume_list)\n",
    "    sorted_doc_list = sorted(zipped_docs, key = lambda x: x[0], reverse=True)\n",
    "    for element in sorted_doc_list:\n",
    "        doc_rating_list = []\n",
    "        doc_rating_list.append(os.path.basename(element[1]))\n",
    "        doc_rating_list.append(\"{:.0%}\".format(element[0]))\n",
    "        final_doc_rating_list.append(doc_rating_list)\n",
    "    return final_doc_rating_list\n",
    "    # print('TF-IDF cosine similarity')\n",
    "    # print(final_doc_rating_list)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "C:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "C:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "C:\\Users\\UNNAT\\.conda\\envs\\resume_ranker_env\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      ":: Resume Compatibility Scores List ::\n",
      "---------------------------------------\n",
      "[['backend_developer_resume_amazon.png', '50%'], ['infra_sec_resume.png', '40%']]\n",
      "\n",
      "\n",
      "------------------------\n",
      "Current Job Description\n",
      "------------------------\n",
      "data scientist ii\n",
      "job id: 1988995 | amazon.com services llc\n",
      "description\n",
      "are you inspired by invention? is problem solving through teamwork in your dna? do you like the idea of seeing how your work impacts the bigger picture? we are a smart team of doers that work passionately to apply cutting edge advances in science and software to solve real-world challenges that will transform our customersâ€™ experiences in ways we canâ€™t even imagine yet. we invent new improvements every day.\n",
      "\n",
      "key job responsibilities\n",
      "this role will be responsible for:\n",
      "\n",
      "thinking big and generating ideas with the data science team and stakeholders.\n",
      "working with customers and cross-functional stakeholder teams to identify, disambiguate, and define problems\n",
      "scoping long-term solutions as a series of smaller, more manageable iterations.\n",
      "creating data science architectures, and building scalable solutions.\n",
      "running simulations, measuring performance, building ml models and designing optimization algorithms\n",
      "supporting existing models, while thinking about next generation solutions.\n",
      "basic qualifications\n",
      "2+ years of experience using scripting languages (e.g. python), and/or statistical/mathematical software (e.g. r, sas, matlab, etc.)\n",
      "prior experience with cloud technologies (preferably aws)\n",
      "comfortable dealing with ambiguity and making headway in the face of uncertainty\n",
      "proven success as both an independent performer and a contributing member of a high-performing team; showing the ability to dive deep and develop innovative ideas for process challenges\n",
      "exposure to best practices in data analytics and reporting; data integrity, data analysis, data mining, design of experiments, and documentation\n",
      "experience in working directly with customers (internal or external), disambiguating problems, scoping requirements, and planning/executing on data science projects.\n",
      "preferred qualifications\n",
      "ms in data science, statistics, operations research, or in highly quantitative field (e.g. computer science, operations research, systems engineering, physics)\n",
      "experience with apache spark/hdfs and deep learning frameworks, such as tensorflow, keras, or mxnet.\n",
      "experience with agile frameworks, e.g, scrum.\n",
      "comfort using a unix-like terminal and shell scripts.\n",
      "prior industry experience.\n",
      "------------------------\n",
      "Current Resumes\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "Resume 0\n",
      "------------------------\n",
      " \n",
      "\n",
      "2\n",
      "\n",
      "tb\n",
      "\n",
      "james ingram\n",
      "\n",
      "555-555-5555 | hello@kickresume.com | www.kickresume.com\n",
      "\n",
      "profile\n",
      "\n",
      "iam a principal data engineer with 15 years of\n",
      "experience working on scalable architectures,\n",
      "distributed computing, big data analytics, micro\n",
      "services, and cloud infrastructures for organizations\n",
      "ranging from start-ups to global enterprises. i excel at\n",
      "team leadership, software development, micro\n",
      "services, cloud computing, and aws.\n",
      "\n",
      "work experience\n",
      "\n",
      "f108/2015 — present\n",
      "\n",
      "principal engineer cloud platform\n",
      "\n",
      "autodesk\n",
      "\n",
      "¢ supervise team of 7 engineers across 4 locations\n",
      "and 3 time zones.\n",
      "\n",
      "¢ lead development of cloud infrastructure\n",
      "supporting autodesk's online electronic design\n",
      "tools.\n",
      "\n",
      "¢ developed microservice architecture currently\n",
      "supporting 8 products.\n",
      "\n",
      "¢ migrated several key apps to cloud with 0 errors or\n",
      "delays.\n",
      "\n",
      "keywords: team management, software development,\n",
      "micro services, cloud computing, aws, nodejs\n",
      "\n",
      "f101/2014 - 08/2015\n",
      "\n",
      "research engineer big data\n",
      "\n",
      "ibm research\n",
      "\n",
      "¢ carried out research on integrating semantic web\n",
      "technologies into big data analytics infrastructure.\n",
      "\n",
      "¢ developed scalable, distributed big data\n",
      "architectures for several global research projects.\n",
      "\n",
      "¢ member of team which developed architecture of\n",
      "ibm's next grand challenge (follow up of ibm's deep\n",
      "blue and watson).\n",
      "\n",
      "keywords: research, big data, semantic web, machine\n",
      "learning, cloud computing, java\n",
      "\n",
      "fj 01/2008 - 12/2013\n",
      "\n",
      "senior researcher semantic web\n",
      "\n",
      "iminds\n",
      "\n",
      "¢ coached semantic web research team, developed\n",
      "research strategy, successfully secured multi-\n",
      "million dollars in funding.\n",
      "\n",
      "e led 2x 5-year eu projects and 13 x 2-year iminds\n",
      "projects.\n",
      "\n",
      "=\n",
      "\n",
      "@\n",
      "\n",
      "work experience\n",
      "\n",
      "¢ participated in standardization activities (w3c:\n",
      "provenance, library of congress: premis owl); acted\n",
      "as ict expert for flemish government.\n",
      "\n",
      "keywords: research, project management, team\n",
      "management, semantic web, java\n",
      "\n",
      "fi 02/2006 - 12/2007\n",
      "\n",
      "r&d engineer 3d printing\n",
      "\n",
      "materialise\n",
      "\n",
      "¢ developed stereo-lithography printers (production\n",
      "3d printers) and control software implemented\n",
      "monitoring services for integration into other\n",
      "business services (iot) increased productivity of\n",
      "printers by 20% while reducing raw material\n",
      "consumption 20% by streamlining process control.\n",
      "\n",
      "keywords: r&d, 3d printing, iot, software development,\n",
      "c++\n",
      "\n",
      " \n",
      "\n",
      "education\n",
      "\n",
      "f105/2013 q ghent, belgium\n",
      "\n",
      "phd computer science semantic web\n",
      "technologies\n",
      "\n",
      "ghent university\n",
      "\n",
      "research topic: provenance information, query\n",
      "federation, and semantic workflow composition and\n",
      "distribution.\n",
      "\n",
      "10 journal articles, 2 books, 3 book chapters, 6 web\n",
      "standard documents, 29 conference papers, 2 iswc best\n",
      "demo awards.\n",
      "\n",
      "&105/2005 9 leuven, belgium\n",
      "\n",
      "msc electrical engineering micro\n",
      "electronics\n",
      "\n",
      "ku leuven\n",
      "\n",
      "thesis described a class e invertor with automatic\n",
      "impedance adaptation for inductive power transfer.\n",
      "\f",
      "\n",
      "\n",
      "------------------------\n",
      "Resume 1\n",
      "------------------------\n",
      " \n",
      "\n",
      "e profile\n",
      "\n",
      "ahmed wayne\n",
      "\n",
      "© date of birth: 11/11/1981 ©@ nationality: egyptian @ address: abu dhabi, uae\n",
      "©@ phone number: +999 99 99 9999 @ email address: hello@kickresume.com\n",
      "\n",
      "a dynamic sr. it infrastructure security specialist. holding a bsc. in electronics engineering, cissp mcse, mcitp\n",
      "vcp cdcp cdcs and itil. with more than 10 years of hands on extensive experience with it infrastructure\n",
      "technologies and information security implementation and operation across private and public clouds. i'm always\n",
      "looking for new ventures where | can apply my expertise, learn and evolve.\n",
      "\n",
      "work experience\n",
      "\n",
      "04/2017 — present\n",
      "dubai, uae\n",
      "\n",
      "06/2013 - 04/2017\n",
      "al ain, uae\n",
      "\n",
      " \n",
      "\n",
      "sr. infrastructure security specialist\n",
      "\n",
      "alzahra hospitals dubai\n",
      "\n",
      "joined the health care group as part of the core his system planning and\n",
      "\n",
      "implementation for 4 hospitals. main responsibilities are:\n",
      "\n",
      "1. evaluate the infrastructure security exposure and readiness for the new his system\n",
      "\n",
      "2. asses infrastructure and application communication channels internally and\n",
      "externally\n",
      "\n",
      "3. review and update the system and application security procedure inline with\n",
      "hippa/dha requirements for the healthcare sector\n",
      "\n",
      "4. implementation of edge security for email communication system (barracuda email\n",
      "security gateway)\n",
      "\n",
      "5. data categorization and evaluation for medical and administrative services. rbac\n",
      "implementation and enforcement\n",
      "\n",
      "6. designing and implementing infrastructure and application hardening methodologies\n",
      "(servers, storage, network, data stores, endpoints, users) including least privileges,\n",
      "firewalls policies, access control, sso, iam, ips, encryption....etc.\n",
      "\n",
      "information security specialist\n",
      "\n",
      "al foah\n",
      "\n",
      "key contributions:\n",
      "\n",
      "1. reviewed and updated the organization information security policies and procedures\n",
      "in compliance with international and governmental standards (iso27k, neesa).\n",
      "\n",
      "2. work with executive management to enforce and apply information security\n",
      "standards\n",
      "\n",
      "3. participate in bia with key business functions heads. defining priorities,\n",
      "dependencies, key systems and tolerable downtime.\n",
      "\n",
      "4. risk assessment. assign and review security controls to mitigate, avoid or accept of\n",
      "the imposed risks.\n",
      "\n",
      "5. dr planning and technology evaluation and testing to achieve required rpos and\n",
      "rtos. | have participated in reaching near zero downtime hot site with continuous\n",
      "replication technology.\n",
      "\n",
      "6. conducted infrastructure review and hardening on different physical and technical\n",
      "evels to achieve required confidentiality, availability and integrity requirements:\n",
      "(cctv, biometric, aaa, strong authentications, firewall, ips, swg, anti-malware, anti-\n",
      "ransomware, backup and recovery, components redundancy and ft, seim,\n",
      "vulnerability management, rbac, patching and baseline enforcement, ha, encryption,\n",
      "ssl, internal and public pki, cloud, saa&, ...etc.)\n",
      "\n",
      "7. developed incident response procedure facilitating detection, communication,\n",
      "contamination, remediation, recovery and reporting. coordinating with national uae\n",
      "cert for notifications and alerting.\n",
      "\n",
      "8. conduct periodic vulnerability scans and pen tests. reporting outcomes for both\n",
      "technical and managerial levels. follow up and participate on remediation plans with\n",
      "nfrastructure and application teams.\n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    req_document = r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\job_description_samples\\amazon_data_scientist_ii.txt'\n",
    "    resume_docs = [\n",
    "        # r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\Unnat_Resume.pdf',\n",
    "        r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\backend_developer_resume_amazon.png',\n",
    "        r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\infra_sec_resume.png'\n",
    "        # r'C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\data_scientist_resume_example.pdf'\n",
    "    ]\n",
    "\n",
    "    result_ranking = process_files(req_document,resume_docs)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\":: Resume Compatibility Scores List ::\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(result_ranking)\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # word_list = ['lead', 'leadership', 'leading']\n",
    "    # op = ' '.join(lemmatizer.lemmatize(w) for w in word_list)\n",
    "    # print(op)\n",
    "\n",
    "    # text = textract.process(r\"C:\\Users\\UNNAT\\UNNAT_CLG\\Project_Resume_Ranking\\resume_ranker\\samples\\resume_samples\\Unnat_Resume.pdf\", method=\"pdfminer\").decode(\"utf8\")\n",
    "    # print(text)\n",
    "       \n",
    "    print(\"\\n\\n------------------------\")\n",
    "    print(\"Current Job Description\")\n",
    "    print(\"------------------------\")\n",
    "    print(get_content_as_string(req_document))\n",
    "    \n",
    "    print(\"------------------------\")\n",
    "    print(\"Current Resumes\")\n",
    "    print(\"------------------------\")\n",
    "    for ind,resume in enumerate(resume_docs):\n",
    "        print(f\"\\n------------------------\\nResume {ind}\\n------------------------\")\n",
    "        print(get_content_as_string(resume))\n",
    "\n",
    "    # TODO : issue with converting pdf to text using textract (fix it) \n",
    "    # https://stackoverflow.com/questions/63359767/textract-failed-with-exit-code-127-pdftotext-on-windows-10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume_ranker_env",
   "language": "python",
   "name": "resume_ranker_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdd37cec0271e5f9301f878c6561a7ffdc9e6fc062c1e94414084ed432e13a60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
